{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, Bidirectional, GRU\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(word_index, max_features):    \n",
    "    embeddings_index = {}\n",
    "    f = codecs.open('word_embeddings/w2v_300.txt', encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('%s word vectors encontrados' % len(embeddings_index))\n",
    "    \n",
    "    words_not_found = []\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # palavras nao encontradas no embedding permanecem com valor nulo\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    \n",
    "    print('Quantidade de word embeddings nulas: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\"', ' ')\n",
    "    text = text.replace('\\'', ' ')    \n",
    "    text = text.replace('<', ' ')\n",
    "    text = text.replace('>', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('*', ' ')\n",
    "    text = text.replace('\\\\', ' ')\n",
    "    text = text.strip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera uma sequência de palavras, a partir de uma sequência anterior\n",
    "# nós usaremos esta função para testar nosso modelo\n",
    "def generate_seq(model, tokenizer, max_length, in_text, n_words):\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        # cria uma sequência de valores inteiros a partir do texto de entrada\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # fixa o tamanho das sequências\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        \n",
    "        # estima a probabilidade para cada palavra da sequência dada\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # esta palavra está presente no índice de palavras do seu vocabulário?\n",
    "        # Se a palavra já foi observada, então vamos\n",
    "        # incluí-la na frase que está sendo gerada.\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # concatena a palavra na sequência sendo gerada\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset.txt', mode='r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 8693\n",
      "Número de sequências: 347761\n",
      "Tamanho máximo da sequência: 3\n"
     ]
    }
   ],
   "source": [
    "data = \"\\n\".join(data)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Tamanho do vocabulário: %d' % vocab_size)\n",
    "\n",
    "# cria uma sequência de valores inteiros a partir do texto de entrada\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "# Aqui, cada palavra estimada está condicionada àquelas palavras\n",
    "# que aparecem antes dela na sequência. Para estimar uma palavra específica,\n",
    "# nós consideramos as 2 palavras que vêm antes dela.\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "    sequence = encoded[i - 2: i + 1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "\n",
    "print('Número de sequências: %d' % len(sequences))\n",
    "\n",
    "# apenas para assegurar que teremos sequências sempre com um mesmo tamanho\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "print('Tamanho máximo da sequência: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "623264it [01:52, 5564.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623264 word vectors encontrados\n",
      "Quantidade de word embeddings nulas: 52\n"
     ]
    }
   ],
   "source": [
    "# carrega nosso word embedding pré-treinado\n",
    "embedding_matrix = load_embeddings(word_index, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 300)            2607900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               105300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8693)              877993    \n",
      "=================================================================\n",
      "Total params: 3,591,193\n",
      "Trainable params: 983,293\n",
      "Non-trainable params: 2,607,900\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/thiago/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/32\n",
      "347761/347761 [==============================] - 171s 491us/step - loss: 3.4698\n",
      "Epoch 2/32\n",
      "347761/347761 [==============================] - 171s 493us/step - loss: 2.7337\n",
      "Epoch 3/32\n",
      "347761/347761 [==============================] - 176s 506us/step - loss: 2.5548\n",
      "Epoch 4/32\n",
      "347761/347761 [==============================] - 206s 592us/step - loss: 2.4543\n",
      "Epoch 5/32\n",
      "347761/347761 [==============================] - 206s 593us/step - loss: 2.3804\n",
      "Epoch 6/32\n",
      "347761/347761 [==============================] - 204s 588us/step - loss: 2.3247\n",
      "Epoch 7/32\n",
      "347761/347761 [==============================] - 201s 577us/step - loss: 2.2774\n",
      "Epoch 8/32\n",
      "347761/347761 [==============================] - 188s 542us/step - loss: 2.2388\n",
      "Epoch 9/32\n",
      "347761/347761 [==============================] - 259s 744us/step - loss: 2.2066\n",
      "Epoch 10/32\n",
      "347761/347761 [==============================] - 231s 664us/step - loss: 2.1150\n",
      "Epoch 14/32\n",
      " 77856/347761 [=====>........................] - ETA: 4:15 - loss: 2.0131"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300, weights=[embedding_matrix], input_length=max_length-1, trainable=False))\n",
    "model.add(Bidirectional(GRU(50)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# treina o nosso modelo.\n",
    "model.fit(X, y, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "input>  embargos de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output: embargos de declaração quando não preenchidos os requisitos previstos no art 896 § 1º a i da\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "input>  agravo de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output: agravo de instrumento em recurso de revista lei 13 015 2014 não observância dos requisitos de admissibilidade\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "input>  conhecido e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output: conhecido e provido agravo de instrumento em recurso de revista lei 13 015 2014 não observância dos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "input>  o exame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output: o exame do recurso de revista lei 13 015 2014 não observância dos requisitos de admissibilidade do\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sentence = input('\\ninput> ')\n",
    "\n",
    "    if sentence == 'exit':\n",
    "        break\n",
    "\n",
    "    print('\\noutput:', generate_seq(model, tokenizer, max_length-1, sentence, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
